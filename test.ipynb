{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d2f9185dbcac85019478d835'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "os.urandom(12).hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:03,830 --> 00:00:06,270\n",
      "hey I just ran into penny she seemed\n",
      "\n",
      "2\n",
      "00:00:06,270 --> 00:00:08,460\n",
      "upset about something I think it's her\n",
      "\n",
      "3\n",
      "00:00:08,460 --> 00:00:13,110\n",
      "time of the month I marked the calendar\n",
      "\n",
      "4\n",
      "00:00:13,110 --> 00:00:18,570\n",
      "for future reference what's with the\n",
      "\n",
      "5\n",
      "00:00:18,570 --> 00:00:24,029\n",
      "fish it's an experiment what happened to\n",
      "\n",
      "6\n",
      "00:00:24,029 --> 00:00:25,920\n",
      "your scrambled egg research oh that was\n",
      "\n",
      "7\n",
      "00:00:25,920 --> 00:00:26,490\n",
      "a dead end\n",
      "\n",
      "8\n",
      "00:00:26,490 --> 00:00:28,199\n",
      "no scrambled eggs are as good as they're\n",
      "\n",
      "9\n",
      "00:00:28,199 --> 00:00:34,530\n",
      "ever going to be so fish I read an\n",
      "\n",
      "10\n",
      "00:00:34,530 --> 00:00:36,329\n",
      "article about Japanese scientists who\n",
      "\n",
      "11\n",
      "00:00:36,329 --> 00:00:38,190\n",
      "inserted DNA from luminous jellyfish\n",
      "\n",
      "12\n",
      "00:00:38,190 --> 00:00:40,280\n",
      "into other animals and I thought hey\n",
      "\n",
      "13\n",
      "00:00:40,280 --> 00:00:47,730\n",
      "fish nightlights fish nightlights it's a\n",
      "\n",
      "14\n",
      "00:00:47,730 --> 00:00:53,750\n",
      "billion-dollar idea mum's the word\n",
      "\n",
      "15\n",
      "00:00:53,750 --> 00:00:55,949\n",
      "Sheldon are you sure you don't want to\n",
      "\n",
      "16\n",
      "00:00:55,949 --> 00:00:57,719\n",
      "just apologize to gablehauser and get\n",
      "\n",
      "17\n",
      "00:00:57,719 --> 00:01:00,149\n",
      "your job back oh no no oh I have too\n",
      "\n",
      "18\n",
      "00:01:00,149 --> 00:01:08,189\n",
      "much to do like luminous fish that's\n",
      "\n",
      "19\n",
      "00:01:08,189 --> 00:01:10,439\n",
      "just the beginning I also have an idea\n",
      "\n",
      "20\n",
      "00:01:10,439 --> 00:01:12,659\n",
      "for a bulk mail-order feminine hygiene\n",
      "\n",
      "21\n",
      "00:01:12,659 --> 00:01:13,950\n",
      "company Oh\n",
      "\n",
      "22\n",
      "00:01:13,950 --> 00:01:19,500\n",
      "glow-in-the-dark tampons Leonard we're\n",
      "\n",
      "23\n",
      "00:01:19,500 --> 00:01:22,220\n",
      "gonna be rich\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./static/video/tbbt1_4.txt') as file:\n",
    "     content = file.read()\n",
    "     print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '00:00:03,830 --> 00:00:06,270', 'hey I just ran into penny she seemed']\n",
      "['00:00:03,830', '00:00:06,270']\n",
      "['00', '00', '06', '270']\n",
      "['2', '00:00:06,270 --> 00:00:08,460', \"upset about something I think it's her\"]\n",
      "['00:00:06,270', '00:00:08,460']\n",
      "['00', '00', '08', '460']\n",
      "['3', '00:00:08,460 --> 00:00:13,110', 'time of the month I marked the calendar']\n",
      "['00:00:08,460', '00:00:13,110']\n",
      "['00', '00', '13', '110']\n",
      "['4', '00:00:13,110 --> 00:00:18,570', \"for future reference what's with the\"]\n",
      "['00:00:13,110', '00:00:18,570']\n",
      "['00', '00', '18', '570']\n",
      "['5', '00:00:18,570 --> 00:00:24,029', \"fish it's an experiment what happened to\"]\n",
      "['00:00:18,570', '00:00:24,029']\n",
      "['00', '00', '24', '029']\n",
      "['6', '00:00:24,029 --> 00:00:25,920', 'your scrambled egg research oh that was']\n",
      "['00:00:24,029', '00:00:25,920']\n",
      "['00', '00', '25', '920']\n",
      "['7', '00:00:25,920 --> 00:00:26,490', 'a dead end']\n",
      "['00:00:25,920', '00:00:26,490']\n",
      "['00', '00', '26', '490']\n",
      "['8', '00:00:26,490 --> 00:00:28,199', \"no scrambled eggs are as good as they're\"]\n",
      "['00:00:26,490', '00:00:28,199']\n",
      "['00', '00', '28', '199']\n",
      "['9', '00:00:28,199 --> 00:00:34,530', 'ever going to be so fish I read an']\n",
      "['00:00:28,199', '00:00:34,530']\n",
      "['00', '00', '34', '530']\n",
      "['10', '00:00:34,530 --> 00:00:36,329', 'article about Japanese scientists who']\n",
      "['00:00:34,530', '00:00:36,329']\n",
      "['00', '00', '36', '329']\n",
      "['11', '00:00:36,329 --> 00:00:38,190', 'inserted DNA from luminous jellyfish']\n",
      "['00:00:36,329', '00:00:38,190']\n",
      "['00', '00', '38', '190']\n",
      "['12', '00:00:38,190 --> 00:00:40,280', 'into other animals and I thought hey']\n",
      "['00:00:38,190', '00:00:40,280']\n",
      "['00', '00', '40', '280']\n",
      "['13', '00:00:40,280 --> 00:00:47,730', \"fish nightlights fish nightlights it's a\"]\n",
      "['00:00:40,280', '00:00:47,730']\n",
      "['00', '00', '47', '730']\n",
      "['14', '00:00:47,730 --> 00:00:53,750', \"billion-dollar idea mum's the word\"]\n",
      "['00:00:47,730', '00:00:53,750']\n",
      "['00', '00', '53', '750']\n",
      "['15', '00:00:53,750 --> 00:00:55,949', \"Sheldon are you sure you don't want to\"]\n",
      "['00:00:53,750', '00:00:55,949']\n",
      "['00', '00', '55', '949']\n",
      "['16', '00:00:55,949 --> 00:00:57,719', 'just apologize to gablehauser and get']\n",
      "['00:00:55,949', '00:00:57,719']\n",
      "['00', '00', '57', '719']\n",
      "['17', '00:00:57,719 --> 00:01:00,149', 'your job back oh no no oh I have too']\n",
      "['00:00:57,719', '00:01:00,149']\n",
      "['00', '01', '00', '149']\n",
      "['18', '00:01:00,149 --> 00:01:08,189', \"much to do like luminous fish that's\"]\n",
      "['00:01:00,149', '00:01:08,189']\n",
      "['00', '01', '08', '189']\n",
      "['19', '00:01:08,189 --> 00:01:10,439', 'just the beginning I also have an idea']\n",
      "['00:01:08,189', '00:01:10,439']\n",
      "['00', '01', '10', '439']\n",
      "['20', '00:01:10,439 --> 00:01:12,659', 'for a bulk mail-order feminine hygiene']\n",
      "['00:01:10,439', '00:01:12,659']\n",
      "['00', '01', '12', '659']\n",
      "['21', '00:01:12,659 --> 00:01:13,950', 'company Oh']\n",
      "['00:01:12,659', '00:01:13,950']\n",
      "['00', '01', '13', '950']\n",
      "['22', '00:01:13,950 --> 00:01:19,500', \"glow-in-the-dark tampons Leonard we're\"]\n",
      "['00:01:13,950', '00:01:19,500']\n",
      "['00', '01', '19', '500']\n",
      "['23', '00:01:19,500 --> 00:01:22,220', 'gonna be rich']\n",
      "['00:01:19,500', '00:01:22,220']\n",
      "['00', '01', '22', '220']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[\\'1\\', \\'00:00:03,830 --> 00:00:06,270\\', \\'hey I just ran into penny she seemed\\']\\n\\n[\\'2\\', \\'00:00:06,270 --> 00:00:08,460\\', \"upset about something I think it\\'s her\"]\\n\\n[\\'3\\', \\'00:00:08,460 --> 00:00:13,110\\', \\'time of the month I marked the calendar\\']\\n\\n[\\'4\\', \\'00:00:13,110 --> 00:00:18,570\\', \"for future reference what\\'s with the\"]\\n\\n[\\'5\\', \\'00:00:18,570 --> 00:00:24,029\\', \"fish it\\'s an experiment what happened to\"]\\n\\n[\\'6\\', \\'00:00:24,029 --> 00:00:25,920\\', \\'your scrambled egg research oh that was\\']\\n\\n[\\'7\\', \\'00:00:25,920 --> 00:00:26,490\\', \\'a dead end\\']\\n\\n[\\'8\\', \\'00:00:26,490 --> 00:00:28,199\\', \"no scrambled eggs are as good as they\\'re\"]\\n\\n[\\'9\\', \\'00:00:28,199 --> 00:00:34,530\\', \\'ever going to be so fish I read an\\']\\n\\n[\\'10\\', \\'00:00:34,530 --> 00:00:36,329\\', \\'article about Japanese scientists who\\']\\n\\n[\\'11\\', \\'00:00:36,329 --> 00:00:38,190\\', \\'inserted DNA from luminous jellyfish\\']\\n\\n[\\'12\\', \\'00:00:38,190 --> 00:00:40,280\\', \\'into other animals and I thought hey\\']\\n\\n[\\'13\\', \\'00:00:40,280 --> 00:00:47,730\\', \"fish nightlights fish nightlights it\\'s a\"]\\n\\n[\\'14\\', \\'00:00:47,730 --> 00:00:53,750\\', \"billion-dollar idea mum\\'s the word\"]\\n\\n[\\'15\\', \\'00:00:53,750 --> 00:00:55,949\\', \"Sheldon are you sure you don\\'t want to\"]\\n\\n[\\'16\\', \\'00:00:55,949 --> 00:00:57,719\\', \\'just apologize to gablehauser and get\\']\\n\\n[\\'17\\', \\'00:00:57,719 --> 00:01:00,149\\', \\'your job back oh no no oh I have too\\']\\n\\n[\\'18\\', \\'00:01:00,149 --> 00:01:08,189\\', \"much to do like luminous fish that\\'s\"]\\n\\n[\\'19\\', \\'00:01:08,189 --> 00:01:10,439\\', \\'just the beginning I also have an idea\\']\\n\\n[\\'20\\', \\'00:01:10,439 --> 00:01:12,659\\', \\'for a bulk mail-order feminine hygiene\\']\\n\\n[\\'21\\', \\'00:01:12,659 --> 00:01:13,950\\', \\'company Oh\\']\\n\\n[\\'22\\', \\'00:01:13,950 --> 00:01:19,500\\', \"glow-in-the-dark tampons Leonard we\\'re\"]\\n\\n[\\'23\\', \\'00:01:19,500 --> 00:01:22,220\\', \\'gonna be rich\\']\\n\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content\n",
    "dict_result = {}\n",
    "def to_ms(tiempo):\n",
    "    hours = int(tiempo[0]) * 3600000\n",
    "    minutes = int(tiempo[1]) * 60000\n",
    "    seconds = int(tiempo[2]) * 1000\n",
    "    miliseconds = seconds + minutes + hours\n",
    "    miliseconds += int(tiempo[2]) % 1000\n",
    "    miliseconds += int(tiempo[3])\n",
    "    return miliseconds\n",
    "def process_match(obj_match):\n",
    "    obj = obj_match[0].split(sep=\"\\n\")\n",
    "    print(obj)\n",
    "    time = obj[1].split(sep=\" --> \")\n",
    "    print(time)\n",
    "    time = time[1].replace(\",\",\":\")\n",
    "    time = time.split(\":\")\n",
    "    print(time)\n",
    "    new_list = []\n",
    "    for i in time:\n",
    "        new_list.append(str(int(i)))\n",
    "    time = \":\".join(new_list)\n",
    "    time = to_ms(new_list)\n",
    "    dict_obj ={\"end\":time,\"data\":obj[2]}\n",
    "    dict_result[obj[0]] = dict_obj\n",
    "    return str(obj)\n",
    "\n",
    "\n",
    "    \n",
    "patron = r'([0-9]{1,2}\\n[0-9][0-9]:[0-9][0-9]:[0-9][0-9],[0-9][0-9][0-9]\\s-->\\s[0-9][0-9]:[0-9][0-9]:[0-9][0-9],[0-9][0-9][0-9]\\n[\\w\\s]*?[^\\n]*)'\n",
    "result = re.sub(patron, process_match,content)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'end': 6276, 'data': 'hey I just ran into penny she seemed'},\n",
       " '2': {'end': 8468, 'data': \"upset about something I think it's her\"},\n",
       " '3': {'end': 13123, 'data': 'time of the month I marked the calendar'},\n",
       " '4': {'end': 18588, 'data': \"for future reference what's with the\"},\n",
       " '5': {'end': 24053, 'data': \"fish it's an experiment what happened to\"},\n",
       " '6': {'end': 25945, 'data': 'your scrambled egg research oh that was'},\n",
       " '7': {'end': 26516, 'data': 'a dead end'},\n",
       " '8': {'end': 28227, 'data': \"no scrambled eggs are as good as they're\"},\n",
       " '9': {'end': 34564, 'data': 'ever going to be so fish I read an'},\n",
       " '10': {'end': 36365, 'data': 'article about Japanese scientists who'},\n",
       " '11': {'end': 38228, 'data': 'inserted DNA from luminous jellyfish'},\n",
       " '12': {'end': 40320, 'data': 'into other animals and I thought hey'},\n",
       " '13': {'end': 47777, 'data': \"fish nightlights fish nightlights it's a\"},\n",
       " '14': {'end': 53803, 'data': \"billion-dollar idea mum's the word\"},\n",
       " '15': {'end': 56004, 'data': \"Sheldon are you sure you don't want to\"},\n",
       " '16': {'end': 57776, 'data': 'just apologize to gablehauser and get'},\n",
       " '17': {'end': 60149, 'data': 'your job back oh no no oh I have too'},\n",
       " '18': {'end': 68197, 'data': \"much to do like luminous fish that's\"},\n",
       " '19': {'end': 70449, 'data': 'just the beginning I also have an idea'},\n",
       " '20': {'end': 72671, 'data': 'for a bulk mail-order feminine hygiene'},\n",
       " '21': {'end': 73963, 'data': 'company Oh'},\n",
       " '22': {'end': 79519, 'data': \"glow-in-the-dark tampons Leonard we're\"},\n",
       " '23': {'end': 82242, 'data': 'gonna be rich'}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education is the most powerful weapon to change the world\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# - - Esta parte del código traduce únicamente un texto plano\n",
    "traductor = GoogleTranslator(source='es', target='en')\n",
    "resultado = traductor.translate(\"La educación es el arma más poderosa para cambiar al mundo\")\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ximi/mango/proybarqui/bc_venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-en-es\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m->: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipe)\n",
      "File \u001b[0;32m~/mango/proybarqui/bc_venv/lib/python3.8/site-packages/transformers/pipelines/__init__.py:1004\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1002\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1004\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mango/proybarqui/bc_venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:848\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 848\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    849\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    850\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    851\u001b[0m             )\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    856\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "tasks= {\n",
    "    \"traductora\":{\n",
    "        \"model\":\"Helsinki-NLP/opus-mt-en-es\",\n",
    "        \"task\":\"translation\"\n",
    "    }\n",
    "}\n",
    "def run_model(task,model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model)\n",
    "    pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "    return pipe\n",
    "i_task = input(\"Task ->: \")\n",
    "result = run_model(i_task)\n",
    "i_data = input(\"->: \")\n",
    "pipe = run_model(task=tasks[i_task]['task'],model=tasks[i_task]['model'])\n",
    "result = pipe(i_data)\n",
    "print(result)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
